## Supplements for Kedro

[pipelinex.extras](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras) provides Kedro hooks, data interface sets, and decorators to supplement [kedro.extras](https://github.com/quantumblacklabs/kedro/tree/develop/kedro/extras) as follows.


### Integration with MLflow by Kedro hooks (callbacks)

  [pipelinex.extras.hooks](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras/hooks) provides Kedro hooks (callbacks) to use MLflow without adding any MLflow-related code in the node (task) functions.

  - [`pipelinex.MLflowBasicLoggerHook`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/hooks/mlflow/mlflow_basic_logger.py): Configures and logs duration time for the pipeline to MLflow with args:
  
    - enable_mlflow: Enable configuring and logging to MLflow.
    - uri: `uri` arg fed to:
        https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.set_tracking_uri
        as the MLflow tracking server URI.
        Local file path, databases supported by SQLAlchemy (sqlite, mysql, mssql, and 
        postgresql), HTTP server, Databricks workspace are supported. 
        See MLflow's document at:
        https://mlflow.org/docs/latest/tracking.html#where-runs-are-recorded
    - experiment_name: `name` arg fed to:
        https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.create_experiment
        as the MLflow experiment name.
    - artifact_location: `artifact_location` arg fed to:
        https://www.mlflow.org/docs/latest/python_api/mlflow.html#mlflow.create_experiment
        as the URI to store the artifacts.
        Local file paths, Amazon S3, Azure Blob Storage, Google Cloud Storage, SFTP server, 
        NFS, and HDFS are supported. 
        See MLflow's document at:
        https://mlflow.org/docs/latest/tracking.html#id10
    - run_name: Shown as 'Run Name' in the MLflow UI.
    - offset_hours: The offset hour (e.g. 0 for UTC+00:00) used for `__time_begin` and `__time_end` parameters. 

  - [`pipelinex.MLflowArtifactsLoggerHook`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/hooks/mlflow/mlflow_artifacts_logger.py): Logs artifacts of specified file paths and dataset names to MLflow with args:

    - enable_mlflow: Enable logging to MLflow.
    - filepaths_before_pipeline_run: The file paths of artifacts to log before the pipeline is run.
    - datasets_after_node_run: The dataset names to log after the node is run.
    - filepaths_after_pipeline_run: The file paths of artifacts to log after the pipeline is run.
  
  - [`pipelinex.MLflowDataSetsLoggerHook`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/hooks/mlflow/mlflow_outputs_logger.py): Logs datasets of (list of) float/int and str classes to MLflow with arg:

    - enable_mlflow: Enable logging to MLflow.
  
  - [`pipelinex.MLflowTimeLoggerHook`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/hooks/mlflow/mlflow_time_logger.py): Logs duration time for each node (task) to MLflow and optionally visualizes the execution logs as a Gantt chart by [`plotly.figure_factory.create_gantt`](https://plotly.github.io/plotly.py-docs/generated/plotly.figure_factory.create_gantt.html) if `plotly` is installed, with args:
    - enable_mlflow: Enable logging to MLflow.
    - enable_plotly: Enable visualization of logged time as a gantt chart.
    - gantt_filepath: File path to save the generated gantt chart.
    - gantt_params: Args fed to:
        https://plotly.github.io/plotly.py-docs/generated/plotly.figure_factory.create_gantt.html
    - metric_name_prefix: Prefix for the metric names. The metric names are
        - `metric_name_prefix` concatenated with the string returned by `task_name_func`.
    - task_name_func: Callable to return the task name using ``kedro.pipeline.node.Node``
        - object.
  
  - [`pipelinex.AddTransformersHook`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/hooks/add_transformers.py): Adds Kedro transformers such as:
    - [`pipelinex.MLflowIOTimeLoggerTransformer`](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/transformers/mlflow/mlflow_io_time_logger.py): Logs duration time to load and save each dataset with args:
      - enable_mlflow: Enable logging to MLflow.
      - metric_name_prefix: Prefix for the metric names. The metric names are `metric_name_prefix` concatenated with 'load <data_set_name>' or 'save <data_set_name>'

  To use these hooks for MLFlow, please use the [Kedro starters](https://github.com/Minyus/kedro-starters-sklearn) which includes the following example:

  ```python
import pipelinex

mlflow_hooks = (
    pipelinex.MLflowBasicLoggerHook(
        enable_mlflow=True,  # Enable configuring and logging to MLflow
        uri="sqlite:///mlruns/sqlite.db",
        experiment_name="experiment_001",
        artifact_location="./mlruns/experiment_001",
        offset_hours=0,  # Specify the offset hour (e.g. 0 for UTC/GMT +00:00) to log in MLflow
    ),  # Configure and log duration time for the pipeline
    pipelinex.MLflowArtifactsLoggerHook(
        enable_mlflow=True,  # Enable logging to MLflow
        filepaths_before_pipeline_run=[
            "conf/base/parameters.yml"
        ],  # Optionally specify the file paths to log before pipeline is run
        filepaths_after_pipeline_run=[
            "data/06_models/model.pkl"
        ],  # Optionally specify the file paths to log after pipeline is run
    ),  # Log artifacts of specified file paths and dataset names
    pipelinex.MLflowDataSetsLoggerHook(
        enable_mlflow=True,  # Enable logging to MLflow
    ),  # Log output datasets of (list of) float, int, and str classes
    pipelinex.MLflowTimeLoggerHook(
        enable_mlflow=True,  # Enable logging to MLflow
    ),  # Log duration time to run each node (task)
    pipelinex.AddTransformersHook(
        transformers=[
            pipelinex.MLflowIOTimeLoggerTransformer(
                enable_mlflow=True
            )  # Log duration time to load and save each dataset
        ],
    ),  # Add transformers
)
  ``` 

<p align="center">
<img src="https://raw.githubusercontent.com/Minyus/pipelinex/master/_doc_images/mlflow_ui_metrics.png">
Logged metrics shown in MLflow's UI
</p>

<p align="center">
<img src="https://raw.githubusercontent.com/Minyus/pipelinex/master/_doc_images/mlflow_ui_gantt.png">
Gantt chart for execution time, generated using Plotly, shown in MLflow's UI
</p>

### Supplements to kedro.extras
[pipelinex.extras](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras) provides features not available in [kedro.extras](https://github.com/quantumblacklabs/kedro/tree/master/kedro/extras).
Contributors who are willing to help preparing the test code and send pull request to Kedro following Kedro's [CONTRIBUTING.md](https://github.com/quantumblacklabs/kedro/blob/master/CONTRIBUTING.md#contribute-a-new-feature) are welcomed.

#### Additional Kedro datasets (data interface sets)
  
[pipelinex.extras.datasets](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras/datasets) provides the following Kedro Datasets (data interface sets) mainly for Computer Vision applications using PyTorch/torchvision, OpenCV, and Scikit-image.

- [pipelinex.ImagesLocalDataSet](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/datasets/pillow/images_dataset.py
)
  - loads/saves multiple numpy arrays (RGB, BGR, or monochrome image) from/to a folder in local storage using `pillow` package, working like ``kedro.extras.datasets.pillow.ImageDataSet`` and
  ``kedro.io.PartitionedDataSet`` with conversion between numpy arrays and Pillow images.
  - an example project is at [pipelinex_image_processing](https://github.com/Minyus/pipelinex_image_processing)
- [pipelinex.APIDataSet](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/datasets/requests/api_dataset.py)
  - modified version of [kedro.extras.APIDataSet](https://github.com/quantumblacklabs/kedro/blob/master/kedro/extras/datasets/api/api_dataset.py) with more flexible options including downloading multiple contents (such as images and json) by HTTP requests to multiple URLs using `requests` package
  - an example project is at [pipelinex_image_processing](https://github.com/Minyus/pipelinex_image_processing)
- [pipelinex.AsyncAPIDataSet](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/datasets/httpx/async_api_dataset.py)
  - downloads multiple contents (such as images and json) by asynchronous HTTP requests to multiple URLs using `httpx` package
  - an example project is at [pipelinex_image_processing](https://github.com/Minyus/pipelinex_image_processing)

- [pipelinex.IterableImagesDataSet](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/datasets/torchvision/iterable_images_dataset.py)
  - wrapper of [`torchvision.datasets.ImageFolder`](https://pytorch.org/docs/stable/torchvision/datasets.html#imagefolder) that loads images in a folder as an iterable data loader to use with PyTorch.

- [pipelinex.PandasProfilingDataSet](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/datasets/pandas_profiling/pandas_profiling.py)
  - generates a pandas dataframe summary report using [pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)

- [more data interface sets for pandas dataframe summarization/visualization provided by PipelineX](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras/datasets)

#### Additional function decorators for benchmarking

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Minyus/pipelinex/blob/master/notebooks/decorators_demo.ipynb)

[pipelinex.extras.decorators](https://github.com/Minyus/pipelinex/tree/master/src/pipelinex/extras/decorators) provides Python decorators for benchmarking.

- [log_time](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/decorators/decorators.py)
  - logs the duration time of a function (difference of timestamp before and after running the function).
  - Slightly modified version of Kedro's [log_time](https://github.com/quantumblacklabs/kedro/blob/develop/kedro/pipeline/decorators.py#L59)

- [mem_profile](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/decorators/memory_profiler.py)
  - logs the peak memory usage during running the function.
  - `memory_profiler` needs to be installed.
  - Slightly modified version of Kedro's [mem_profile](https://github.com/quantumblacklabs/kedro/blob/develop/kedro/extras/decorators/memory_profiler.py#L48)

- [nvml_profile](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/extras/decorators/nvml_profiler.py)
  - logs the difference of NVIDIA GPU usage before and after running the function.
  - `pynvml` or `py3nvml` needs to be installed.

```python
from pipelinex import log_time
from pipelinex import mem_profile  # Need to install memory_profiler for memory profiling
from pipelinex import nvml_profile  # Need to install pynvml for NVIDIA GPU profiling
from time import sleep
import logging

logging.basicConfig(level=logging.INFO)

@nvml_profile
@mem_profile
@log_time
def foo_func(i=1):
    sleep(0.5)  # Needed to avoid the bug reported at https://github.com/pythonprofilers/memory_profiler/issues/216
    return "a" * i

output = foo_func(100_000_000)
```

```
INFO:pipelinex.decorators.decorators:Running 'foo_func' took 549ms [0.549s]
INFO:pipelinex.decorators.memory_profiler:Running 'foo_func' consumed 579.02MiB memory at peak time
INFO:pipelinex.decorators.nvml_profiler:Ran: 'foo_func', NVML returned: {'_Driver_Version': '418.67', '_NVML_Version': '10.418.67', 'Device_Count': 1, 'Devices': [{'_Name': 'Tesla P100-PCIE-16GB', 'Total_Memory': 17071734784, 'Free_Memory': 17071669248, 'Used_Memory': 65536, 'GPU_Utilization_Rate': 0, 'Memory_Utilization_Rate': 0}]}, Used memory diff: [0]
```


