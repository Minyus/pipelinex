## Enhanced Kedro context: YAML interface for Kedro pipelines

PipelineX enables you to use Kedro in more convenient ways.
Using [pipelinex.FlexibleContext](https://github.com/Minyus/pipelinex/blob/master/src/pipelinex/framework/context/flexible_context.py), you can define the inter-task dependency (DAG) for Kedro pipelines in YAML.

### Here are the options configurable in `parameters.yml`:

#### `HatchDict` features

```yaml
# parameters.yml

model:
  =: sklearn.linear_model.LogisticRegression
  C: 1.23456
  max_iter: 987
  random_state: 42
cols_features: # Columns used as features in the Titanic data table
  - Pclass # The passenger's ticket class
  - Parch # # of parents / children aboard the Titanic
col_target: Survived # Column used as the target: whether the passenger survived or not
```

#### Define Kedro pipelines using `PIPELINES` key
  - Optionally specify the default Python module (path of .py file) if you want to omit the module name
  - Optionally specify the Python function decorator to apply to each node
  - Specify `inputs`, `func`, and `outputs` for each node
    - For sub-pipelines consisting of nodes of only single input and single output, you can optionally use Sequential API similar to PyTorch (`torch.nn.Sequential`) and Keras (`tf.keras.Sequential`)

```yaml
# parameters.yml

PIPELINES:
  __default__:
    =: pipelinex.FlexiblePipeline
    module: # Optionally specify the default Python module so you can omit the module name to which functions belongs
    decorator: # Optionally specify function decorator(s) to apply to each node
    nodes:
      - inputs: ["params:model", train_df, "params:cols_features", "params:col_target"]
        func: sklearn_demo.train_model
        outputs: model

      - inputs: [model, test_df, "params:cols_features"]
        func: sklearn_demo.run_inference
        outputs: pred_df
```

#### Configure Kedro run config using `RUN_CONFIG` key
  - Optionally run nodes in parallel
  - Optionally run only missing nodes (skip tasks which have already been run to resume pipeline using the intermediate data files or databases.)
  Note: You can use Kedro CLI to overwrite these run configs.

```yaml
# parameters.yml

RUN_CONFIG:
  pipeline_name: __default__
  runner: SequentialRunner # Set to "ParallelRunner" to run in parallel
  only_missing: False # Set True to run only missing nodes
  tags: # None
  node_names: # None
  from_nodes: # None
  to_nodes: # None
  from_inputs: # None
  load_versions: # None
```

#### Define Kedro hooks using `HOOKS` key (Note: This option works with Kedro 0.16.x only and will be deprecated)

```yaml
# parameters.yml

HOOKS:
  - =: pipelinex.MLflowBasicLoggerHook # Configure and log duration time for the pipeline 
    enable_mlflow: True # Enable configuring and logging to MLflow
    uri: sqlite:///mlruns/sqlite.db
    experiment_name: experiment_001
    artifact_location: ./mlruns/experiment_001
    offset_hours: 0 # Specify the offset hour (e.g. 0 for UTC/GMT +00:00) to log in MLflow

  - =: pipelinex.MLflowArtifactsLoggerHook # Log artifacts of specified file paths and dataset names
    enable_mlflow: True # Enable logging to MLflow
    filepaths_before_pipeline_run: # Optionally specify the file paths to log before pipeline is run
      - conf/base/parameters.yml
    datasets_after_node_run: # Optionally specify the dataset names to log after the node is run
      - model
    filepaths_after_pipeline_run: # None  # Optionally specify the file paths to log after pipeline is run

  - =: pipelinex.MLflowOutputsLoggerHook # Log output datasets of (list of) float, int, and str classes
    enable_mlflow: True # Enable logging to MLflow

  - =: pipelinex.MLflowTimeLoggerHook # Log duration time to run each node (task)
    enable_mlflow: True # Enable logging to MLflow

  - =: pipelinex.AddTransformersHook # Add transformers
    transformers: 
      =: pipelinex.MLflowIOTimeLoggerTransformer # Log duration time to load and save each dataset
      enable_mlflow: True
```


### Options available in `catalog.yml`

- `HatchDict` features available
- Optionally enable caching using `cached` key set to True if you do not want Kedro to load the data from disk/database which were in the memory. ([`kedro.io.CachedDataSet`](https://kedro.readthedocs.io/en/latest/kedro.io.CachedDataSet.html#kedro.io.CachedDataSet) is used under the hood.)



